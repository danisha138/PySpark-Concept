

Q10. What would inferSchema do while reading a file?
inferSchema scan the input file and assign the schema to data frame.
By default inferSchema is false, you can enforce it by setting it
true while reading the data.

It can slowdown the loading if data size is too large.
It is preferrable to define schema instead to avoid any surprises.
This could be beneficial if you are input data schema is dynamic and keep on changing.


Q11. What is the need for broadcast variables in Spark?

Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.

They can be used to give every node a copy of a large input dataset in an efficient manner.

Spark distributes broadcast variables using efficient broadcast algorithms to reduce communication costs.


Q12. What are the different ways to define schema and why it is recommended.

Ans: There are two ways to define the schema
Programmatic Manner: In this approach we define the schema of the dataframe in coded manner. 
Using the programming syntax classes and object

Declarative Manner: In this approach we can define the schema of the dataframe in kind of SQL manner.


#Define schema progarmatically

2 3 from pyspark.sql.types import

orderSchema = StructType([StructField("Region", StringType ,StructField("Country", StringType(),True) 4

,StructField("ItemType", StringType(),True)

5 6 ,StructField("SalesChannel", StringType(), True)

7 , StructField("OrderPriority", StringType(), True)

8 9 ,StructField("OrderID", Integer Type(), True) ,StructField("Units Sold", Integer Type(), True)

10 , StructField("UnitPrice", DoubleType(),True)

11 , StructField("UnitCost", DoubleType(), True)

13

12 ,StructField("TotalRevenue", DoubleType(), True) ,StructField("TotalCost", DoubleType(), True)

14

15

16

17

18

df = spark. read. load ("/FileStore/tables/Order.csv", format="csv" header-True, schema-orderSchema)

19

df.printSchema()

df.schema



Define schema Declaratively

orderSchema Region String,Country String,ItemType String, SalesChannel String OrderPriority String,Order ID Integer itssald tager ,UnitCost Double TotalRevenue Double,TotalCost Double, TotalProfit Double
df= spark.read. load("/FileStore/tables/Order.csv", format="csv", header True, schema-orderschena)
df.printSchema()



Q15. What is Spark SQL?

Ans: Spark SQL is a Spark module for structured data processing

It provides abstraction called CataFrames and can also act as a distributed SQL query engine.

Using Spark SQL it is possible to do the transformation using the SQL

Helps data analyst and data people to take advantage to spark without knowing it much.
