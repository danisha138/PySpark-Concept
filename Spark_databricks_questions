

Q10. What would inferSchema do while reading a file?
inferSchema scan the input file and assign the schema to data frame.
By default inferSchema is false, you can enforce it by setting it
true while reading the data.

It can slowdown the loading if data size is too large.
It is preferrable to define schema instead to avoid any surprises.
This could be beneficial if you are input data schema is dynamic and keep on changing.


Q11. What is the need for broadcast variables in Spark?

Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.

They can be used to give every node a copy of a large input dataset in an efficient manner.

Spark distributes broadcast variables using efficient broadcast algorithms to reduce communication costs.


Q12. What are the different ways to define schema and why it is recommended.

Ans: There are two ways to define the schema
Programmatic Manner: In this approach we define the schema of the dataframe in coded manner. 
Using the programming syntax classes and object

Declarative Manner: In this approach we can define the schema of the dataframe in kind of SQL manner.


#Define schema progarmatically

2 3 from pyspark.sql.types import

orderSchema = StructType([StructField("Region", StringType ,StructField("Country", StringType(),True) 4

,StructField("ItemType", StringType(),True)

5 6 ,StructField("SalesChannel", StringType(), True)

7 , StructField("OrderPriority", StringType(), True)

8 9 ,StructField("OrderID", Integer Type(), True) ,StructField("Units Sold", Integer Type(), True)

10 , StructField("UnitPrice", DoubleType(),True)

11 , StructField("UnitCost", DoubleType(), True)

13

12 ,StructField("TotalRevenue", DoubleType(), True) ,StructField("TotalCost", DoubleType(), True)

14

15

16

17

18

df = spark. read. load ("/FileStore/tables/Order.csv", format="csv" header-True, schema-orderSchema)

19

df.printSchema()

df.schema
