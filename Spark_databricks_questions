

Q10. What would inferSchema do while reading a file?
inferSchema scan the input file and assign the schema to data frame.
By default inferSchema is false, you can enforce it by setting it
true while reading the data.

It can slowdown the loading if data size is too large.
It is preferrable to define schema instead to avoid any surprises.
This could be beneficial if you are input data schema is dynamic and keep on changing.


Q11. What is the need for broadcast variables in Spark?

Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.

They can be used to give every node a copy of a large input dataset in an efficient manner.

Spark distributes broadcast variables using efficient broadcast algorithms to reduce communication costs.


Q12. What are the different ways to define schema and why it is recommended.

Ans: There are two ways to define the schema
Programmatic Manner: In this approach we define the schema of the dataframe in coded manner. 
Using the programming syntax classes and object

Declarative Manner: In this approach we can define the schema of the dataframe in kind of SQL manner.


#Define schema progarmatically

2 3 from pyspark.sql.types import

orderSchema = StructType([StructField("Region", StringType ,StructField("Country", StringType(),True) 4

,StructField("ItemType", StringType(),True)

5 6 ,StructField("SalesChannel", StringType(), True)

7 , StructField("OrderPriority", StringType(), True)

8 9 ,StructField("OrderID", Integer Type(), True) ,StructField("Units Sold", Integer Type(), True)

10 , StructField("UnitPrice", DoubleType(),True)

11 , StructField("UnitCost", DoubleType(), True)

13

12 ,StructField("TotalRevenue", DoubleType(), True) ,StructField("TotalCost", DoubleType(), True)

14

15

16

17

18

df = spark. read. load ("/FileStore/tables/Order.csv", format="csv" header-True, schema-orderSchema)

19

df.printSchema()

df.schema



Define schema Declaratively

orderSchema Region String,Country String,ItemType String, SalesChannel String OrderPriority String,Order ID Integer itssald tager ,UnitCost Double TotalRevenue Double,TotalCost Double, TotalProfit Double
df= spark.read. load("/FileStore/tables/Order.csv", format="csv", header True, schema-orderschena)
df.printSchema()



Q15. What is Spark SQL?
Ans: Spark SQL is a Spark module for structured data processing

It provides abstraction called CataFrames and can also act as a distributed SQL query engine.

Using Spark SQL it is possible to do the transformation using the SQL

Helps data analyst and data people to take advantage to spark without knowing it much.



Q16. How to create temporary views?
Ans: To apply operations on dataframe in the SQL manner, temporary view can be created.

Use function createOrReplaceTempView("view_name") on dataframe.
createTempView("view_name")

Example:
df.createOrReplace TempView("Customer")
spark.sql("select * from Customer").show()

TEMPORARY views are session-scoped and is dropped when session ends 
because it skips persisting the definition in the underlying metastore, if any


Q17. How to create global views?
Ans: Use function createOrReplaceGlobalTempView("view_name") on dataframe.

Example of createOrReplaceGlobalTempView(Customer)
spark.sql("select from global temp Customer").show()

GLOBAL TEMPORARY views are tied to a system preserved temporary schema global_temp.

This feature is useful when you want to share data among different session
and keep alive until your application ends.


Q19. What are the magic commands in Databricks? When to use?
There are multiple magic commands.

%python, %scala, %r, %sql: You can put this as the first line in the cell to run that in a specific language irrespective of the notebook type.

%sh, %fs: You can use to run the shell command or the file system
command within the specific cell.

%md: To convert specific cell as markdown cell. To add the static text content.

%run: To run the one notebook from inside the other notebook.


Q20. How would you call a notebook from another notebook?
Ans: You can call a notebook from an another notebook using the %run command.

Example could be to define helper functions in one notebook that are used by other notebooks.

%run must be in a cell by itself, because it runs the entire notebook inline.

%run ./ChildNotebook


Q24. What are the different types of clusters there in Databricks.

Interactive Cluster
1.Used to run interactive queries
2.Used for development environment
3.Manually need to start stop cluster

Job Cluster
1.Use to execute the jobs autmatically
2.Used for Production environment
3.Automatically get started and stop with job.


Q25. What are the different Cluster mode in Databricks
Ans: Databricks supports three cluster modes: Standard, High Concurrency, and Single Node. The default cluster mode is Standard.

Standard clusters:
A Standard cluster is recommended for a single user, 
Standard clusters can run workloads developed in any language: Python, SQL, R, and Scala.

High Concurrency clusters:
A High Concurrency cluster is a managed cloud resource. 
It provide fine- grained sharing for maximum resource utilization and minimum query latencies.

The performance and security of High Concurrency clusters is provided
by running user code in separate processes.

In addition, only High Concurrency clusters support table access control.


Single Node clusters:
A Single Node cluster has no workers and runs Spark jobs on the driver node.
In contrast, a Standard cluster requires at least one Spark worker node in addition to the driver node to execute Spark jobs.
