✳️ Spark as ETL (Extract-- Transform -- Load) ✳️

✳️ Spark Extraction using READ API
**********************************
✅ creating dataframe using spark read api
✅ df = spark.read.api (csv,json,parquet,orc,text....)
✅ df = spark.createDataFrame(data,schema)
✅ df = spark.read.format().load() (format - csv,json,xml,excel,....)

✳️ Spark Transform using Metadata and Data Transformations
************************************************************
✅ df = df.withColumn() - for adding new column or chaging data type
✅ df = df.withColumnRenamed() - for renaming a column
✅ df = df.drop() - removing a column or columns
✅ df = df.toDF() - renaming multiple columns
✅ df = df.dropDuplicates() - for removing duplicates
✅ df = df.dropna(how='all') - for removing null rows
✅ df = df.fillna({col:val,col2:val}) - for converting null values into actual values
✅ df = df.filter() - for filtering rows based on condition
✅ df = df.select() - for selecting required columns
✅ df = df.selectExpr() - for selecting columns with expressions
✅ df = df.groupBy().agg() - for grouping and aggregations
✅ df = df.orderBy() - for global sorting
✅ df = df.sort() - for local sorting

✳️ Spark Load using Write API
*****************************
✅ Loading data into table or File using write API
✅ df.write.format("csv").mode("append").saveAsTable(table) -- table
✅ df.write.format("csv").mode("append").save(path) -- file
✅ df.write.format("csv").mode("overwrite").saveAsTable(table)
✅ df.write.format("csv").mode("overwrite").save(path) -- file
